{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import tolls.io\n",
    "import tools.opts\n",
    "from itertools import chain\n",
    "import io\n",
    "import codecs\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torchtext.data\n",
    "import torchtext.vocab\n",
    "\n",
    "from collections import namedtuple\n",
    "import os\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from itertools import count\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_WORD = '<blank>'\n",
    "UNK_WORD = '<unk>'\n",
    "UNK = 0\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "\n",
    "\n",
    "class ONMTDatasetBase(torchtext.data.Dataset):\n",
    "    \"\"\"\n",
    "    A dataset basically supports iteration over all the examples\n",
    "    it contains.\n",
    "    Internally it initializes an `torchtext.data.Dataset` object with\n",
    "    the following attributes:\n",
    "     `examples`: a sequence of `torchtext.data.Example` objects.\n",
    "     `fields`: a dictionary associating str keys with `torchtext.data.Field`\n",
    "    objects, and not necessarily having the same keys as the input fields.\n",
    "    \"\"\"\n",
    "    def __getstate__(self):\n",
    "        return self.__dict__\n",
    "\n",
    "    def __setstate__(self, d):\n",
    "        self.__dict__.update(d)\n",
    "\n",
    "    def __reduce_ex__(self, proto):\n",
    "        \"This is a hack. Something is broken with torch pickle.\"\n",
    "        return super(ONMTDatasetBase, self).__reduce_ex__()\n",
    "\n",
    "    def load_fields(self, vocab_dict):\n",
    "        \"\"\" Load fields from vocab.pt, and set the `fields` attribute.\n",
    "        Args:\n",
    "            vocab_dict (dict): a dict of loaded vocab from vocab.pt file.\n",
    "        \"\"\"\n",
    "\n",
    "        fields = load_fields_from_vocab(vocab_dict.items(), self.data_type)\n",
    "        self.fields = dict([(k, f) for (k, f) in fields.items()\n",
    "                           if k in self.examples[0].__dict__])\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_text_features(tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokens: A list of tokens, where each token consists of a word,\n",
    "                optionally followed by u\"￨\"-delimited features.\n",
    "        Returns:\n",
    "            A sequence of words, a sequence of features, and num of features.\n",
    "        \"\"\"\n",
    "        if not tokens:\n",
    "            return [], [], -1\n",
    "\n",
    "        split_tokens = [token.split(u\"￨\") for token in tokens]\n",
    "        split_tokens = [token for token in split_tokens if token[0]]\n",
    "        token_size = len(split_tokens[0])\n",
    "#         print(f\"split_tokens {split_tokens}\")\n",
    "#         print(f\"split_tokens {split_tokens[0]}\")\n",
    "\n",
    "        assert all(len(token) == token_size for token in split_tokens), \\\n",
    "            \"all words must have the same number of features\"\n",
    "        words_and_features = list(zip(*split_tokens))\n",
    "        words = words_and_features[0]\n",
    "        features = words_and_features[1:]\n",
    "\n",
    "        return words, features, token_size - 1\n",
    "\n",
    "    # Below are helper functions for intra-class use only.\n",
    "\n",
    "    def _join_dicts(self, *args):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dictionaries with disjoint keys.\n",
    "        Returns:\n",
    "            a single dictionary that has the union of these keys.\n",
    "        \"\"\"\n",
    "        return dict(chain(*[d.items() for d in args]))\n",
    "\n",
    "    def _peek(self, seq):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            seq: an iterator.\n",
    "        Returns:\n",
    "            the first thing returned by calling next() on the iterator\n",
    "            and an iterator created by re-chaining that value to the beginning\n",
    "            of the iterator.\n",
    "        \"\"\"\n",
    "        first = next(seq)\n",
    "        return first, chain([first], seq)\n",
    "\n",
    "    def _construct_example_fromlist(self, data, fields):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: the data to be set as the value of the attributes of\n",
    "                the to-be-created `Example`, associating with respective\n",
    "                `Field` objects with same key.\n",
    "            fields: a dict of `torchtext.data.Field` objects. The keys\n",
    "                are attributes of the to-be-created `Example`.\n",
    "        Returns:\n",
    "            the created `Example` object.\n",
    "        \"\"\"\n",
    "        ex = torchtext.data.Example()\n",
    "        for (name, field), val in zip(fields, data):\n",
    "            if field is not None:\n",
    "                setattr(ex, name, field.preprocess(val))\n",
    "            else:\n",
    "                setattr(ex, name, val)\n",
    "        return ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aeq(*args):\n",
    "    \"\"\"\n",
    "    Assert all arguments have the same value\n",
    "    \"\"\"\n",
    "    arguments = (arg for arg in args)\n",
    "    first = next(arguments)\n",
    "    assert all(arg == first for arg in arguments), \\\n",
    "        \"Not all arguments have the same value: \" + str(args)\n",
    "\n",
    "\n",
    "\n",
    "class TextDataset(ONMTDatasetBase):\n",
    "    \"\"\" Dataset for data_type=='text'\n",
    "        Build `Example` objects, `Field` objects, and filter_pred function\n",
    "        from text corpus.\n",
    "        Args:\n",
    "            fields (dict): a dictionary of `torchtext.data.Field`.\n",
    "                Keys are like 'src', 'tgt', 'src_map', and 'alignment'.\n",
    "            src_examples_iter (dict iter): preprocessed source example\n",
    "                dictionary iterator.\n",
    "            tgt_examples_iter (dict iter): preprocessed target example\n",
    "                dictionary iterator.\n",
    "            num_src_feats (int): number of source side features.\n",
    "            num_tgt_feats (int): number of target side features.\n",
    "            src_seq_length (int): maximum source sequence length.\n",
    "            tgt_seq_length (int): maximum target sequence length.\n",
    "            dynamic_dict (bool): create dynamic dictionaries?\n",
    "            use_filter_pred (bool): use a custom filter predicate to filter\n",
    "                out examples?\n",
    "    \"\"\"\n",
    "    def __init__(self, fields, src_examples_iter, tgt_examples_iter,\n",
    "                 num_src_feats=0, num_tgt_feats=0,\n",
    "                 src_seq_length=0, tgt_seq_length=0,\n",
    "                 dynamic_dict=True, use_filter_pred=True):\n",
    "        self.data_type = 'text'\n",
    "\n",
    "        # self.src_vocabs: mutated in dynamic_dict, used in\n",
    "        # collapse_copy_scores and in Translator.py\n",
    "        self.src_vocabs = []\n",
    "\n",
    "        self.n_src_feats = num_src_feats\n",
    "        self.n_tgt_feats = num_tgt_feats\n",
    "\n",
    "        # Each element of an example is a dictionary whose keys represents\n",
    "        # at minimum the src tokens and their indices and potentially also\n",
    "        # the src and tgt features and alignment information.\n",
    "        if tgt_examples_iter is not None:\n",
    "            examples_iter = (self._join_dicts(src, tgt) for src, tgt in\n",
    "                             zip(src_examples_iter, tgt_examples_iter))\n",
    "        else:\n",
    "            examples_iter = src_examples_iter\n",
    "\n",
    "        if dynamic_dict:\n",
    "            examples_iter = self._dynamic_dict(examples_iter)\n",
    "\n",
    "        # Peek at the first to see which fields are used.\n",
    "        ex, examples_iter = self._peek(examples_iter)\n",
    "        keys = ex.keys()\n",
    "\n",
    "        out_fields = [(k, fields[k]) if k in fields else (k, None)\n",
    "                      for k in keys]\n",
    "        example_values = ([ex[k] for k in keys] for ex in examples_iter)\n",
    "\n",
    "        # If out_examples is a generator, we need to save the filter_pred\n",
    "        # function in serialization too, which would cause a problem when\n",
    "        # `torch.save()`. Thus we materialize it as a list.\n",
    "        src_size = 0\n",
    "        tgt_size = 0\n",
    "        out_examples = []\n",
    "        for ex_values in example_values:\n",
    "            example = self._construct_example_fromlist(\n",
    "                ex_values, out_fields)\n",
    "            src_size += len(example.src)\n",
    "            out_examples.append(example)\n",
    "            #tgt_size += len(example.tgt)\n",
    "        print(\"average src size\", src_size / len(out_examples),\n",
    "              len(out_examples))\n",
    "        #print(\"Number of target tokens: {}\".format(tgt_size))\n",
    "\n",
    "        def filter_pred(example):\n",
    "            return 0 < len(example.src) <= src_seq_length \\\n",
    "               and 0 < len(example.tgt) <= tgt_seq_length\n",
    "\n",
    "        filter_pred = filter_pred if use_filter_pred else lambda x: True\n",
    "\n",
    "        super(TextDataset, self).__init__(\n",
    "            out_examples, out_fields, filter_pred\n",
    "        )\n",
    "\n",
    "    def sort_key(self, ex):\n",
    "        \"\"\" Sort using length of source sentences. \"\"\"\n",
    "        # Default to a balanced sort, prioritizing tgt len match.\n",
    "        # TODO: make this configurable.\n",
    "        if hasattr(ex, \"tgt\"):\n",
    "            return len(ex.src), len(ex.tgt)\n",
    "        return len(ex.src)\n",
    "\n",
    "    @staticmethod\n",
    "    def collapse_copy_scores(scores, batch, tgt_vocab, src_vocabs):\n",
    "        \"\"\"\n",
    "        Given scores from an expanded dictionary\n",
    "        corresponeding to a batch, sums together copies,\n",
    "        with a dictionary word when it is ambigious.\n",
    "        \"\"\"\n",
    "        offset = len(tgt_vocab)\n",
    "        for b in range(batch.batch_size):\n",
    "            blank = []\n",
    "            fill = []\n",
    "            index = batch.indices.data[b]\n",
    "            src_vocab = src_vocabs[index]\n",
    "            for i in range(1, len(src_vocab)):\n",
    "                sw = src_vocab.itos[i]\n",
    "                ti = tgt_vocab.stoi[sw]\n",
    "                if ti != 0:\n",
    "                    blank.append(offset + i)\n",
    "                    fill.append(ti)\n",
    "            if blank:\n",
    "                blank = torch.Tensor(blank).type_as(batch.indices.data)\n",
    "                fill = torch.Tensor(fill).type_as(batch.indices.data)\n",
    "                scores[:, b].index_add_(1, fill,\n",
    "                                        scores[:, b].index_select(1, blank))\n",
    "                scores[:, b].index_fill_(1, blank, 1e-10)\n",
    "        return scores\n",
    "\n",
    "    @staticmethod\n",
    "    def make_text_examples_nfeats_tpl(path, truncate, side):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): location of a src or tgt file.\n",
    "            truncate (int): maximum sequence length (0 for unlimited).\n",
    "            side (str): \"src\" or \"tgt\".\n",
    "        Returns:\n",
    "            (example_dict iterator, num_feats) tuple.\n",
    "        \"\"\"\n",
    "        assert side in ['src', 'tgt']\n",
    "\n",
    "        if path is None:\n",
    "            return (None, 0)\n",
    "\n",
    "        # All examples have same number of features, so we peek first one\n",
    "        # to get the num_feats.\n",
    "        examples_nfeats_iter = \\\n",
    "            TextDataset.read_text_file(path, truncate, side)\n",
    "\n",
    "        first_ex = next(examples_nfeats_iter)\n",
    "        num_feats = first_ex[1]\n",
    "\n",
    "        # Chain back the first element - we only want to peek it.\n",
    "        examples_nfeats_iter = chain([first_ex], examples_nfeats_iter)\n",
    "        examples_iter = (ex for ex, nfeats in examples_nfeats_iter)\n",
    "\n",
    "        return (examples_iter, num_feats)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_text_file(path, truncate, side):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            path (str): location of a src or tgt file.\n",
    "            truncate (int): maximum sequence length (0 for unlimited).\n",
    "            side (str): \"src\" or \"tgt\".\n",
    "        Yields:\n",
    "            (word, features, nfeat) triples for each line.\n",
    "        \"\"\"\n",
    "        with codecs.open(path, \"r\", \"utf-8\") as corpus_file:\n",
    "            for i, line in enumerate(corpus_file):\n",
    "                line = line.strip().split()\n",
    "                if truncate:\n",
    "                    line = line[:truncate]\n",
    "\n",
    "                words, feats, n_feats = \\\n",
    "                    TextDataset.extract_text_features(line)\n",
    "\n",
    "                example_dict = {side: words, \"indices\": i}\n",
    "                if feats:\n",
    "                    prefix = side + \"_feat_\"\n",
    "                    example_dict.update((prefix + str(j), f)\n",
    "                                        for j, f in enumerate(feats))\n",
    "                yield example_dict, n_feats\n",
    "\n",
    "    @staticmethod\n",
    "    def get_fields(n_src_features, n_tgt_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_src_features (int): the number of source features to\n",
    "                create `torchtext.data.Field` for.\n",
    "            n_tgt_features (int): the number of target features to\n",
    "                create `torchtext.data.Field` for.\n",
    "        Returns:\n",
    "            A dictionary whose keys are strings and whose values\n",
    "            are the corresponding Field objects.\n",
    "        \"\"\"\n",
    "        fields = {}\n",
    "\n",
    "        fields[\"src\"] = torchtext.data.Field(\n",
    "            pad_token=PAD_WORD,\n",
    "            include_lengths=True)\n",
    "\n",
    "        for j in range(n_src_features):\n",
    "            fields[\"src_feat_\"+str(j)] = \\\n",
    "                torchtext.data.Field(pad_token=PAD_WORD)\n",
    "\n",
    "        fields[\"tgt\"] = torchtext.data.Field(\n",
    "            init_token=BOS_WORD, eos_token=EOS_WORD,\n",
    "            pad_token=PAD_WORD)\n",
    "\n",
    "        for j in range(n_tgt_features):\n",
    "            fields[\"tgt_feat_\"+str(j)] = \\\n",
    "                torchtext.data.Field(init_token=BOS_WORD, eos_token=EOS_WORD,\n",
    "                                     pad_token=PAD_WORD)\n",
    "\n",
    "        def make_src(data, vocab):\n",
    "            src_size = max([t.size(0) for t in data])\n",
    "            src_vocab_size = max([t.max() for t in data]) + 1\n",
    "            alignment = torch.zeros(src_size, len(data), src_vocab_size)\n",
    "            for i, sent in enumerate(data):\n",
    "                for j, t in enumerate(sent):\n",
    "                    alignment[j, i, t] = 1\n",
    "            return alignment\n",
    "\n",
    "        fields[\"src_map\"] = torchtext.data.Field(\n",
    "            use_vocab=False, dtype=torch.float,\n",
    "            postprocessing=make_src, sequential=False)\n",
    "\n",
    "        def make_tgt(data, vocab):\n",
    "            tgt_size = max([t.size(0) for t in data])\n",
    "            alignment = torch.zeros(tgt_size, len(data)).long()\n",
    "            for i, sent in enumerate(data):\n",
    "                alignment[:sent.size(0), i] = sent\n",
    "            return alignment\n",
    "\n",
    "        fields[\"alignment\"] = torchtext.data.Field(\n",
    "            use_vocab=False, dtype=torch.long,\n",
    "            postprocessing=make_tgt, sequential=False)\n",
    "\n",
    "        fields[\"indices\"] = torchtext.data.Field(\n",
    "            use_vocab=False, dtype=torch.long,\n",
    "            sequential=False)\n",
    "\n",
    "        return fields\n",
    "\n",
    "    @staticmethod\n",
    "    def get_num_features(corpus_file, side):\n",
    "        \"\"\"\n",
    "        Peek one line and get number of features of it.\n",
    "        (All lines must have same number of features).\n",
    "        For text corpus, both sides are in text form, thus\n",
    "        it works the same.\n",
    "        Args:\n",
    "            corpus_file (str): file path to get the features.\n",
    "            side (str): 'src' or 'tgt'.\n",
    "        Returns:\n",
    "            number of features on `side`.\n",
    "        \"\"\"\n",
    "        \n",
    "        with codecs.open(corpus_file, \"r\", \"utf-8\") as cf:\n",
    "            f_line = cf.readline().strip().split()\n",
    "            \n",
    "            _, _, num_feats = TextDataset.extract_text_features(f_line)\n",
    "\n",
    "        return num_feats\n",
    "\n",
    "    # Below are helper functions for intra-class use only.\n",
    "    def _dynamic_dict(self, examples_iter):\n",
    "        for example in examples_iter:\n",
    "            src = example[\"src\"]\n",
    "            src_vocab = torchtext.vocab.Vocab(Counter(src),\n",
    "                                              specials=[UNK_WORD, PAD_WORD])\n",
    "            self.src_vocabs.append(src_vocab)\n",
    "            # Mapping source tokens to indices in the dynamic dict.\n",
    "            src_map = torch.LongTensor([src_vocab.stoi[w] for w in src])\n",
    "            example[\"src_map\"] = src_map\n",
    "\n",
    "            if \"tgt\" in example:\n",
    "                tgt = example[\"tgt\"]\n",
    "                mask = torch.LongTensor(\n",
    "                    [0] + [src_vocab.stoi[w] for w in tgt] + [0])\n",
    "                example[\"alignment\"] = mask\n",
    "            yield example\n",
    "\n",
    "\n",
    "class ShardedTextCorpusIterator(object):\n",
    "    \"\"\"\n",
    "    This is the iterator for text corpus, used for sharding large text\n",
    "    corpus into small shards, to avoid hogging memory.\n",
    "    Inside this iterator, it automatically divides the corpus file into\n",
    "    shards of size `shard_size`. Then, for each shard, it processes\n",
    "    into (example_dict, n_features) tuples when iterates.\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus_path, line_truncate, side, shard_size,\n",
    "                 assoc_iter=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            corpus_path: the corpus file path.\n",
    "            line_truncate: the maximum length of a line to read.\n",
    "                            0 for unlimited.\n",
    "            side: \"src\" or \"tgt\".\n",
    "            shard_size: the shard size, 0 means not sharding the file.\n",
    "            assoc_iter: if not None, it is the associate iterator that\n",
    "                        this iterator should align its step with.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # The codecs module seems to have bugs with seek()/tell(),\n",
    "            # so we use io.open().\n",
    "            self.corpus = io.open(corpus_path, \"r\", encoding=\"utf-8\")\n",
    "        except IOError:\n",
    "            sys.stderr.write(\"Failed to open corpus file: %s\" % corpus_path)\n",
    "            sys.exit(1)\n",
    "\n",
    "        self.line_truncate = line_truncate\n",
    "        self.side = side\n",
    "        self.shard_size = shard_size\n",
    "        self.assoc_iter = assoc_iter\n",
    "        self.last_pos = 0\n",
    "        self.line_index = -1\n",
    "        self.eof = False\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterator of (example_dict, nfeats).\n",
    "        On each call, it iterates over as many (example_dict, nfeats) tuples\n",
    "        until this shard's size equals to or approximates `self.shard_size`.\n",
    "        \"\"\"\n",
    "        iteration_index = -1\n",
    "        if self.assoc_iter is not None:\n",
    "            # We have associate iterator, just yields tuples\n",
    "            # util we run parallel with it.\n",
    "            while self.line_index < self.assoc_iter.line_index:\n",
    "                line = self.corpus.readline()\n",
    "                if line == '':\n",
    "                    raise AssertionError(\n",
    "                        \"Two corpuses must have same number of lines!\")\n",
    "\n",
    "                self.line_index += 1\n",
    "                iteration_index += 1\n",
    "                yield self._example_dict_iter(line, iteration_index)\n",
    "\n",
    "            if self.assoc_iter.eof:\n",
    "                self.eof = True\n",
    "                self.corpus.close()\n",
    "        else:\n",
    "            # Yield tuples util this shard's size reaches the threshold.\n",
    "            self.corpus.seek(self.last_pos)\n",
    "            while True:\n",
    "                if self.shard_size != 0 and self.line_index % 64 == 0:\n",
    "                    # This part of check is time consuming on Py2 (but\n",
    "                    # it is quite fast on Py3, weird!). So we don't bother\n",
    "                    # to check for very line. Instead we chekc every 64\n",
    "                    # lines. Thus we are not dividing exactly per\n",
    "                    # `shard_size`, but it is not too much difference.\n",
    "                    cur_pos = self.corpus.tell()\n",
    "                    if cur_pos >= self.last_pos + self.shard_size:\n",
    "                        self.last_pos = cur_pos\n",
    "                        raise StopIteration\n",
    "\n",
    "                line = self.corpus.readline()\n",
    "                if line == '':\n",
    "                    self.eof = True\n",
    "                    self.corpus.close()\n",
    "                    raise StopIteration\n",
    "\n",
    "                self.line_index += 1\n",
    "                iteration_index += 1\n",
    "                yield self._example_dict_iter(line, iteration_index)\n",
    "\n",
    "    def hit_end(self):\n",
    "        return self.eof\n",
    "\n",
    "    @property\n",
    "    def num_feats(self):\n",
    "        # We peek the first line and seek back to\n",
    "        # the beginning of the file.\n",
    "        saved_pos = self.corpus.tell()\n",
    "\n",
    "        line = self.corpus.readline().split()\n",
    "        if self.line_truncate:\n",
    "            line = line[:self.line_truncate]\n",
    "        _, _, self.n_feats = TextDataset.extract_text_features(line)\n",
    "\n",
    "        self.corpus.seek(saved_pos)\n",
    "\n",
    "        return self.n_feats\n",
    "\n",
    "    def _example_dict_iter(self, line, index):\n",
    "        line = line.split()\n",
    "        if self.line_truncate:\n",
    "            line = line[:self.line_truncate]\n",
    "        words, feats, n_feats = TextDataset.extract_text_features(line)\n",
    "        example_dict = {self.side: words, \"indices\": index}\n",
    "        if feats:\n",
    "            # All examples must have same number of features.\n",
    "            aeq(self.n_feats, n_feats)\n",
    "\n",
    "            prefix = self.side + \"_feat_\"\n",
    "            example_dict.update((prefix + str(j), f)\n",
    "                                for j, f in enumerate(feats))\n",
    "\n",
    "        return example_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _getstate(self):\n",
    "    return dict(self.__dict__, stoi=dict(self.stoi))\n",
    "\n",
    "\n",
    "def _setstate(self, state):\n",
    "    self.__dict__.update(state)\n",
    "    self.stoi = defaultdict(lambda: 0, self.stoi)\n",
    "\n",
    "\n",
    "torchtext.vocab.Vocab.__getstate__ = _getstate\n",
    "torchtext.vocab.Vocab.__setstate__ = _setstate\n",
    "\n",
    "\n",
    "def get_fields(data_type, n_src_features, n_tgt_features):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_type: type of the source input. Options are [text|img|audio].\n",
    "        n_src_features: the number of source features to\n",
    "            create `torchtext.data.Field` for.\n",
    "        n_tgt_features: the number of target features to\n",
    "            create `torchtext.data.Field` for.\n",
    "    Returns:\n",
    "        A dictionary whose keys are strings and whose values are the\n",
    "        corresponding Field objects.\n",
    "    \"\"\"\n",
    "    return TextDataset.get_fields(n_src_features, n_tgt_features)\n",
    "\n",
    "\n",
    "def load_fields_from_vocab(vocab, data_type=\"text\"):\n",
    "    \"\"\"\n",
    "    Load Field objects from `vocab.pt` file.\n",
    "    \"\"\"\n",
    "    vocab = dict(vocab)\n",
    "    n_src_features = len(collect_features(vocab, 'src'))\n",
    "    n_tgt_features = len(collect_features(vocab, 'tgt'))\n",
    "    fields = get_fields(data_type, n_src_features, n_tgt_features)\n",
    "    for k, v in vocab.items():\n",
    "        # Hack. Can't pickle defaultdict :(\n",
    "        v.stoi = defaultdict(lambda: 0, v.stoi)\n",
    "        fields[k].vocab = v\n",
    "    return fields\n",
    "\n",
    "\n",
    "def save_fields_to_vocab(fields):\n",
    "    \"\"\"\n",
    "    Save Vocab objects in Field objects to `vocab.pt` file.\n",
    "    \"\"\"\n",
    "    vocab = []\n",
    "    for k, f in fields.items():\n",
    "        if f is not None and 'vocab' in f.__dict__:\n",
    "            f.vocab.stoi = dict(f.vocab.stoi)\n",
    "            vocab.append((k, f.vocab))\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def merge_vocabs(vocabs, vocab_size=None):\n",
    "    \"\"\"\n",
    "    Merge individual vocabularies (assumed to be generated from disjoint\n",
    "    documents) into a larger vocabulary.\n",
    "    Args:\n",
    "        vocabs: `torchtext.vocab.Vocab` vocabularies to be merged\n",
    "        vocab_size: `int` the final vocabulary size. `None` for no limit.\n",
    "    Return:\n",
    "        `torchtext.vocab.Vocab`\n",
    "    \"\"\"\n",
    "    merged = sum([vocab.freqs for vocab in vocabs], Counter())\n",
    "    return torchtext.vocab.Vocab(merged,\n",
    "                                 specials=[UNK_WORD, PAD_WORD,\n",
    "                                           BOS_WORD, EOS_WORD],\n",
    "                                 max_size=vocab_size)\n",
    "\n",
    "\n",
    "def get_num_features(data_type, corpus_file, side):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data_type (str): type of the source input.\n",
    "            Options are [text|img|audio].\n",
    "        corpus_file (str): file path to get the features.\n",
    "        side (str): for source or for target.\n",
    "    Returns:\n",
    "        number of features on `side`.\n",
    "    \"\"\"\n",
    "    assert side in [\"src\", \"tgt\"]\n",
    "\n",
    "    if data_type == 'text':\n",
    "        \n",
    "        return TextDataset.get_num_features(corpus_file, side)\n",
    "\n",
    "\n",
    "def make_features(batch, side, data_type='text'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        batch (Variable): a batch of source or target data.\n",
    "        side (str): for source or for target.\n",
    "        data_type (str): type of the source input.\n",
    "            Options are [text|img|audio].\n",
    "    Returns:\n",
    "        A sequence of src/tgt tensors with optional feature tensors\n",
    "        of size (len x batch).\n",
    "    \"\"\"\n",
    "    assert side in ['src', 'tgt']\n",
    "    if isinstance(batch.__dict__[side], tuple):\n",
    "        data = batch.__dict__[side][0]\n",
    "    else:\n",
    "        data = batch.__dict__[side]\n",
    "\n",
    "    feat_start = side + \"_feat_\"\n",
    "    keys = sorted([k for k in batch.__dict__ if feat_start in k])\n",
    "    features = [batch.__dict__[k] for k in keys]\n",
    "    levels = [data] + features\n",
    "\n",
    "    if data_type == 'text':\n",
    "        return torch.cat([level.unsqueeze(2) for level in levels], 2)\n",
    "    else:\n",
    "        return levels[0]\n",
    "\n",
    "\n",
    "def collect_features(fields, side=\"src\"):\n",
    "    \"\"\"\n",
    "    Collect features from Field object.\n",
    "    \"\"\"\n",
    "    assert side in [\"src\", \"tgt\"]\n",
    "    feats = []\n",
    "    for j in count():\n",
    "        key = side + \"_feat_\" + str(j)\n",
    "        if key not in fields:\n",
    "            break\n",
    "        feats.append(key)\n",
    "    return feats\n",
    "\n",
    "\n",
    "def collect_feature_vocabs(fields, side):\n",
    "    \"\"\"\n",
    "    Collect feature Vocab objects from Field object.\n",
    "    \"\"\"\n",
    "    assert side in ['src', 'tgt']\n",
    "    feature_vocabs = []\n",
    "    for j in count():\n",
    "        key = side + \"_feat_\" + str(j)\n",
    "        if key not in fields:\n",
    "            break\n",
    "        feature_vocabs.append(fields[key].vocab)\n",
    "    return feature_vocabs\n",
    "\n",
    "\n",
    "def build_dataset(fields, data_type, src_path, tgt_path, src_dir=None,\n",
    "                  src_seq_length=0, tgt_seq_length=0,\n",
    "                  src_seq_length_trunc=0, tgt_seq_length_trunc=0,\n",
    "                  dynamic_dict=True, sample_rate=0,\n",
    "                  window_size=0, window_stride=0, window=None,\n",
    "                  normalize_audio=True, use_filter_pred=True):\n",
    "\n",
    "    # Build src/tgt examples iterator from corpus files, also extract\n",
    "    # number of features.\n",
    "    src_examples_iter, num_src_feats = \\\n",
    "        _make_examples_nfeats_tpl(data_type, src_path, src_dir,\n",
    "                                  src_seq_length_trunc, sample_rate,\n",
    "                                  window_size, window_stride,\n",
    "                                  window, normalize_audio)\n",
    "\n",
    "    # For all data types, the tgt side corpus is in form of text.\n",
    "    tgt_examples_iter, num_tgt_feats = \\\n",
    "        TextDataset.make_text_examples_nfeats_tpl(\n",
    "            tgt_path, tgt_seq_length_trunc, \"tgt\")\n",
    "\n",
    "    if data_type == 'text':\n",
    "        dataset = TextDataset(fields, src_examples_iter, tgt_examples_iter,\n",
    "                              num_src_feats, num_tgt_feats,\n",
    "                              src_seq_length=src_seq_length,\n",
    "                              tgt_seq_length=tgt_seq_length,\n",
    "                              dynamic_dict=dynamic_dict,\n",
    "                              use_filter_pred=use_filter_pred)\n",
    "\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def _build_field_vocab(field, counter, **kwargs):\n",
    "    specials = list(OrderedDict.fromkeys(\n",
    "        tok for tok in [field.unk_token, field.pad_token, field.init_token,\n",
    "                        field.eos_token]\n",
    "        if tok is not None))\n",
    "    field.vocab = field.vocab_cls(counter, specials=specials, **kwargs)\n",
    "\n",
    "\n",
    "def build_vocab(train_dataset_files, fields, data_type, share_vocab,\n",
    "                src_vocab_path, src_vocab_size, src_words_min_frequency,\n",
    "                tgt_vocab_path, tgt_vocab_size, tgt_words_min_frequency):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_dataset_files: a list of train dataset pt file.\n",
    "        fields (dict): fields to build vocab for.\n",
    "        data_type: \"text\", \"img\" or \"audio\"?\n",
    "        share_vocab(bool): share source and target vocabulary?\n",
    "        src_vocab_path(string): Path to src vocabulary file.\n",
    "        src_vocab_size(int): size of the source vocabulary.\n",
    "        src_words_min_frequency(int): the minimum frequency needed to\n",
    "                include a source word in the vocabulary.\n",
    "        tgt_vocab_path(string): Path to tgt vocabulary file.\n",
    "        tgt_vocab_size(int): size of the target vocabulary.\n",
    "        tgt_words_min_frequency(int): the minimum frequency needed to\n",
    "                include a target word in the vocabulary.\n",
    "    Returns:\n",
    "        Dict of Fields\n",
    "    \"\"\"\n",
    "    counter = {}\n",
    "    for k in fields:\n",
    "        counter[k] = Counter()\n",
    "\n",
    "    # Load vocabulary\n",
    "    src_vocab = None\n",
    "    if len(src_vocab_path) > 0:\n",
    "        src_vocab = set([])\n",
    "        print('Loading source vocab from %s' % src_vocab_path)\n",
    "        assert os.path.exists(src_vocab_path), \\\n",
    "            'src vocab %s not found!' % src_vocab_path\n",
    "        with open(src_vocab_path) as f:\n",
    "            for line in f:\n",
    "                word = line.strip().split()[0]\n",
    "                src_vocab.add(word)\n",
    "\n",
    "    tgt_vocab = None\n",
    "    if len(tgt_vocab_path) > 0:\n",
    "        tgt_vocab = set([])\n",
    "        print('Loading target vocab from %s' % tgt_vocab_path)\n",
    "        assert os.path.exists(tgt_vocab_path), \\\n",
    "            'tgt vocab %s not found!' % tgt_vocab_path\n",
    "        with open(tgt_vocab_path) as f:\n",
    "            for line in f:\n",
    "                word = line.strip().split()[0]\n",
    "                tgt_vocab.add(word)\n",
    "\n",
    "    for path in train_dataset_files:\n",
    "        dataset = torch.load(path)\n",
    "        print(\" * reloading %s.\" % path)\n",
    "        for ex in dataset.examples:\n",
    "            for k in fields:\n",
    "                val = getattr(ex, k, None)\n",
    "                if val is not None and not fields[k].sequential:\n",
    "                    val = [val]\n",
    "                elif k == 'src' and src_vocab:\n",
    "                    val = [item for item in val if item in src_vocab]\n",
    "                elif k == 'tgt' and tgt_vocab:\n",
    "                    val = [item for item in val if item in tgt_vocab]\n",
    "                counter[k].update(val)\n",
    "\n",
    "    _build_field_vocab(fields[\"tgt\"], counter[\"tgt\"],\n",
    "                       max_size=tgt_vocab_size,\n",
    "                       min_freq=tgt_words_min_frequency)\n",
    "    print(\" * tgt vocab size: %d.\" % len(fields[\"tgt\"].vocab))\n",
    "\n",
    "    # All datasets have same num of n_tgt_features,\n",
    "    # getting the last one is OK.\n",
    "    for j in range(dataset.n_tgt_feats):\n",
    "        key = \"tgt_feat_\" + str(j)\n",
    "        _build_field_vocab(fields[key], counter[key])\n",
    "        print(\" * %s vocab size: %d.\" % (key, len(fields[key].vocab)))\n",
    "\n",
    "    if data_type == 'text':\n",
    "        _build_field_vocab(fields[\"src\"], counter[\"src\"],\n",
    "                           max_size=src_vocab_size,\n",
    "                           min_freq=src_words_min_frequency)\n",
    "        print(\" * src vocab size: %d.\" % len(fields[\"src\"].vocab))\n",
    "\n",
    "        # All datasets have same num of n_src_features,\n",
    "        # getting the last one is OK.\n",
    "        for j in range(dataset.n_src_feats):\n",
    "            key = \"src_feat_\" + str(j)\n",
    "            _build_field_vocab(fields[key], counter[key])\n",
    "            print(\" * %s vocab size: %d.\" % (key, len(fields[key].vocab)))\n",
    "\n",
    "        # Merge the input and output vocabularies.\n",
    "        if share_vocab:\n",
    "            # `tgt_vocab_size` is ignored when sharing vocabularies\n",
    "            print(\" * merging src and tgt vocab...\")\n",
    "            merged_vocab = merge_vocabs(\n",
    "                [fields[\"src\"].vocab, fields[\"tgt\"].vocab],\n",
    "                vocab_size=src_vocab_size)\n",
    "            fields[\"src\"].vocab = merged_vocab\n",
    "            fields[\"tgt\"].vocab = merged_vocab\n",
    "\n",
    "    return fields\n",
    "\n",
    "\n",
    "def _make_examples_nfeats_tpl(data_type, src_path, src_dir,\n",
    "                              src_seq_length_trunc, sample_rate,\n",
    "                              window_size, window_stride,\n",
    "                              window, normalize_audio):\n",
    "    \"\"\"\n",
    "    Process the corpus into (example_dict iterator, num_feats) tuple\n",
    "    on source side for different 'data_type'.\n",
    "    \"\"\"\n",
    "\n",
    "    if data_type == 'text':\n",
    "        src_examples_iter, num_src_feats = \\\n",
    "            TextDataset.make_text_examples_nfeats_tpl(\n",
    "                src_path, src_seq_length_trunc, \"src\")\n",
    "\n",
    "    return src_examples_iter, num_src_feats\n",
    "\n",
    "\n",
    "class OrderedIterator(torchtext.data.Iterator):\n",
    "    def create_batches(self):\n",
    "        if self.train:\n",
    "            def pool(data, random_shuffler):\n",
    "                for p in torchtext.data.batch(data, self.batch_size * 100):\n",
    "                    p_batch = torchtext.data.batch(\n",
    "                        sorted(p, key=self.sort_key),\n",
    "                        self.batch_size, self.batch_size_fn)\n",
    "                    for b in random_shuffler(list(p_batch)):\n",
    "                        yield b\n",
    "            self.batches = pool(self.data(), self.random_shuffler)\n",
    "        else:\n",
    "            self.batches = []\n",
    "            for b in torchtext.data.batch(self.data(), self.batch_size,\n",
    "                                          self.batch_size_fn):\n",
    "                self.batches.append(sorted(b, key=self.sort_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_save_text_dataset_in_shards(src_corpus, tgt_corpus, fields,\n",
    "                                      corpus_type, max_shard_size=0,\n",
    "                                     src_seq_length_trunc=0, tgt_seq_length_trunc=0,\n",
    "                                     src_seq_length=125, tgt_seq_length=125,dynamic_dict=True,\n",
    "                                     leave_valid=True, save_data='data/iwslt/iwslt_125'):\n",
    "    '''\n",
    "    Divide the big corpus into shards, and build dataset separately.\n",
    "    This is currently only for data_type=='text'.\n",
    "    The reason we do this is to avoid taking up too much memory due\n",
    "    to sucking in a huge corpus file.\n",
    "    To tackle this, we only read in part of the corpus file of size\n",
    "    `max_shard_size`(actually it is multiples of 64 bytes that equals\n",
    "    or is slightly larger than this size), and process it into dataset,\n",
    "    then write it to disk along the way. By doing this, we only focus on\n",
    "    part of the corpus at any moment, thus effectively reducing memory use.\n",
    "    According to test, this method can reduce memory footprint by ~50%.\n",
    "    Note! As we process along the shards, previous shards might still\n",
    "    stay in memory, but since we are done with them, and no more\n",
    "    reference to them, if there is memory tight situation, the OS could\n",
    "    easily reclaim these memory.\n",
    "    If `max_shard_size` is 0 or is larger than the corpus size, it is\n",
    "    effectively preprocessed into one dataset, i.e. no sharding.\n",
    "    NOTE! `max_shard_size` is measuring the input corpus size, not the\n",
    "    output pt file size. So a shard pt file consists of examples of size\n",
    "    2 * `max_shard_size`(source + target).\n",
    "    '''\n",
    "\n",
    "    corpus_size = os.path.getsize(src_corpus)\n",
    "    if corpus_size > 10 * (1024**2) and max_shard_size == 0:\n",
    "        print(\"Warning. The corpus %s is larger than 10M bytes, you can \"\n",
    "              \"set '-max_shard_size' to process it by small shards \"\n",
    "              \"to use less memory.\" % src_corpus)\n",
    "\n",
    "    if max_shard_size != 0:\n",
    "        print(' * divide corpus into shards and build dataset separately'\n",
    "              '(shard_size = %d bytes).' % max_shard_size)\n",
    "\n",
    "    ret_list = []\n",
    "    src_iter = ShardedTextCorpusIterator(\n",
    "        src_corpus, src_seq_length_trunc,\n",
    "        \"src\", max_shard_size)\n",
    "    tgt_iter = ShardedTextCorpusIterator(\n",
    "        tgt_corpus, tgt_seq_length_trunc,\n",
    "        \"tgt\", max_shard_size,\n",
    "        assoc_iter=src_iter)\n",
    "\n",
    "    index = 0\n",
    "    while not src_iter.hit_end():\n",
    "        index += 1\n",
    "        dataset = TextDataset(\n",
    "            fields, src_iter, tgt_iter,\n",
    "            src_iter.num_feats, tgt_iter.num_feats,\n",
    "            src_seq_length=src_seq_length,\n",
    "            tgt_seq_length=tgt_seq_length,\n",
    "            dynamic_dict=dynamic_dict,\n",
    "            use_filter_pred=corpus_type == \"train\" or not leave_valid)\n",
    "\n",
    "        # We save fields in vocab.pt seperately, so make it empty.\n",
    "        dataset.fields = []\n",
    "\n",
    "        pt_file = \"{:s}.{:s}.{:d}.pt\".format(\n",
    "            save_data, corpus_type, index)\n",
    "        print(\" * saving %s data shard to %s.\" % (corpus_type, pt_file))\n",
    "        torch.save(dataset, pt_file)\n",
    "\n",
    "        ret_list.append(pt_file)\n",
    "\n",
    "    return ret_list\n",
    "\n",
    "\n",
    "def build_save_dataset(corpus_type, fields, valid_src_corpus='data/iwslt14-de-en/valid.de.bpe', \n",
    "                       valid_tgt_corpus='data/iwslt14-de-en/valid.en.bpe', save_data='data/iwslt/iwslt_125',\n",
    "                      leave_valid=True):\n",
    "    \n",
    "    assert corpus_type in ['train', 'valid']\n",
    "\n",
    "    if corpus_type == 'train':\n",
    "        src_corpus = 'data/iwslt14-de-en/train.de.bpe'\n",
    "        tgt_corpus = 'data/iwslt14-de-en/train.en.bpe'\n",
    "    else:\n",
    "        src_corpus = valid_src_corpus\n",
    "        tgt_corpus = valid_tgt_corpus\n",
    "\n",
    "    return build_save_text_dataset_in_shards(\n",
    "            src_corpus, tgt_corpus, fields,\n",
    "            corpus_type, save_data=save_data, leave_valid=leave_valid)\n",
    "\n",
    "\n",
    "def build_save_vocab(train_dataset, fields, data_type, share_vocab, src_vocab, src_vocab_size, \n",
    "                    src_words_min_frequency, tgt_vocab, tgt_vocab_size, tgt_words_min_frequency, save_data):\n",
    "    \n",
    "    fields = build_vocab(train_dataset, fields, data_type,\n",
    "                         share_vocab, \n",
    "                         src_vocab, src_vocab_size, src_words_min_frequency,\n",
    "                         tgt_vocab, tgt_vocab_size, tgt_words_min_frequency)\n",
    "\n",
    "    # Can't save fields, so remove/reconstruct at training time.\n",
    "    vocab_file = save_data + '.vocab.pt'\n",
    "    torch.save(save_fields_to_vocab(fields), vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = 'text'\n",
    "train_src = 'data/iwslt14-de-en/train.de.bpe'\n",
    "train_tgt = 'data/iwslt14-de-en/train.en.bpe'\n",
    "\n",
    "valid_src = 'data/iwslt14-de-en/valid.de.bpe'\n",
    "valid_tgt = 'data/iwslt14-de-en/valid.en.bpe'\n",
    "\n",
    "test_src = 'data/iwslt14-de-en/test.de.bpe'\n",
    "test_tgt = 'data/iwslt14-de-en/test.en.bpe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_nfeats = get_num_features(data_type, train_src, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_nfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_nfeats = get_num_features(data_type, train_tgt, 'tgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_nfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = get_fields(data_type, src_nfeats, tgt_nfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning. The corpus data/iwslt14-de-en/train.de.bpe is larger than 10M bytes, you can set '-max_shard_size' to process it by small shards to use less memory.\n",
      "average src size 23.135330649439815 160215\n",
      " * saving train data shard to data/iwslt/iwslt_125.train.1.pt.\n"
     ]
    }
   ],
   "source": [
    "train_dataset_files = build_save_dataset('train', fields, save_data='data/iwslt/iwslt_125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * reloading data/iwslt/iwslt_125.train.1.pt.\n",
      " * tgt vocab size: 8876.\n",
      " * src vocab size: 11987.\n",
      " * merging src and tgt vocab...\n"
     ]
    }
   ],
   "source": [
    "build_save_vocab(train_dataset_files, fields, data_type, share_vocab=True, \n",
    "                 src_vocab=\"\", src_vocab_size=80000, src_words_min_frequency=0, \n",
    "                 tgt_vocab=\"\", tgt_vocab_size=80000, tgt_words_min_frequency=0, \n",
    "                 save_data='data/iwslt/iwslt_125')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average src size 23.3503158472947 7282\n",
      " * saving valid data shard to data/iwslt/iwslt_125.valid.1.pt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/iwslt/iwslt_125.valid.1.pt']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_save_dataset('valid', fields, valid_src_corpus='data/iwslt14-de-en/valid.de.bpe',\n",
    "                   valid_tgt_corpus='data/iwslt14-de-en/valid.en.bpe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning. The corpus data/iwslt14-de-en/train.de.bpe is larger than 10M bytes, you can set '-max_shard_size' to process it by small shards to use less memory.\n",
      "average src size 23.135330649439815 160215\n",
      " * saving train data shard to data/iwslt/iwslt_125_test.train.1.pt.\n"
     ]
    }
   ],
   "source": [
    "train_dataset_files2 = build_save_dataset('train', fields, save_data='data/iwslt/iwslt_125_test', leave_valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * reloading data/iwslt/iwslt_125_test.train.1.pt.\n",
      " * tgt vocab size: 8876.\n",
      " * src vocab size: 11987.\n",
      " * merging src and tgt vocab...\n"
     ]
    }
   ],
   "source": [
    "build_save_vocab(train_dataset_files2, fields, data_type, share_vocab=True, \n",
    "                 src_vocab=\"\", src_vocab_size=80000, src_words_min_frequency=0, \n",
    "                 tgt_vocab=\"\", tgt_vocab_size=80000, tgt_words_min_frequency=0, \n",
    "                 save_data='data/iwslt/iwslt_125_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average src size 22.080296296296297 6750\n",
      " * saving valid data shard to data/iwslt/iwslt_125_test.valid.1.pt.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['data/iwslt/iwslt_125_test.valid.1.pt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_save_dataset('valid', fields, valid_src_corpus='data/iwslt14-de-en/test.de.bpe',\n",
    "                   valid_tgt_corpus='data/iwslt14-de-en/test.en.bpe', save_data='data/iwslt/iwslt_125_test',\n",
    "                  leave_valid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset from data/iwslt/iwslt_125.train.1.pt, number of examples: 160042\n"
     ]
    }
   ],
   "source": [
    "first_dataset = next(lazily_load_dataset(\"train\"))\n",
    "data_type = first_dataset.data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}